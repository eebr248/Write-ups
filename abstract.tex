\chapter*{Abstract}
Research in visuo-motor coupling has shown that the matching of visual and proprioceptive information is important for calibration. Many state-of-the art \textit{Virtual Reality} (VR) systems, commonly known as \textit{Immersive Virtual Environments} (IVE), are created for training users in tasks that require accurate manual dexterity. Unfortunately, these systems can suffer from technical limitations that may force de-coupling of visual and proprioceptive information due to interference, latency, and tracking error. It has also been suggested that closed-loop feedback of travel and locomotion in an IVE can overcome compression of visually perceived depth in medium field distances in the virtual world ~\cite{KCT13,MCT06}. Very few experiments have examined the carryover effects of multi-sensory feedback in IVEs during manual dexterous 3D user interaction in overcoming distortions in near-field or interaction space depth perception, and the relative importance of visual and proprioceptive information in calibrating users’ distance judgments. In the first part of this work, we examined the recalibration of movements when the visually reached distance is scaled differently than the physically reached distance. We present an empirical evaluation of how visually distorted movements affects users' reach to near field targets %using a closed-loop physical reach task 
in an IVE. 

In a between subjects design, participants provided manual reaching distance estimates during three sessions; a baseline measure without feedback (open-loop distance estimation), a calibration session with visual and proprioceptive feedback (closed-loop distance estimation), and a post-interaction session without feedback (open-loop distance estimation). Subjects were randomly assigned to one of three visual feedbacks in the closed-loop condition during which they reached to target while holding a tracked stylus: i) Minus condition (-20\% gain condition) in which the visual stylus appeared at 80\% of the distance of the physical stylus, ii) Neutral condition (0\% or no gain condition) in which the visual stylus was co-located with the physical stylus, and iii) Plus condition (+20\% gain condition) in which the visual stylus appeared at 120\% of the distance of the physical stylus. In all the conditions, there is evidence of visuo-motor calibration in that users' accuracy in physically reaching to the target locations improved over trials. Scaled visual feedback was shown to calibrate distance judgments within an IVE, with estimates being farthest in the post-interaction session after calibrating to visual information appearing nearer (Minus condition), and nearest after calibrating to visual information appearing further (Plus condition). The same pattern was observed during closed-loop physical reach responses, participants generally tended to physically reach farther in Minus condition and closer in Plus condition to the perceived location of the targets, as compared to Neutral condition in which participants' physical reach was more accurate to the perceived location of the target.
 
We also characterized the properties of human reach motion in the presence or absence of visuo-haptic feedback in real and IVEs within a participant's maximum arm reach. Our goal is to understand how physical reaching actions to the perceived location of targets in the presence or absence of visuo-haptic feedback are different between real and virtual viewing conditions. Typically, participants reach to the perceived location of objects in the 3D environment to perform selection and manipulation actions during 3D interaction in applications such as virtual assembly or rehabilitation. In these tasks, participants typically have distorted perceptual information in the IVE as compared to the real world, in part due to technological limitations such as minimal visual field of view, resolution, latency and jitter. In an empirical evaluation, we asked the following questions; i) how do the perceptual differences between virtual and real world affect our ability to accurately reach to the locations of 3D objects, and ii) how do the motor responses of participants differ between the presence or absence of visual and haptic feedback? We examined factors such as velocity and distance of physical reaching behavior between the real world and IVE, both in the presence or absence of visuo-haptic information. The results suggest that physical reach responses vary systematically between real and virtual environments especially in situations involving presence or absence of visuo-haptic feedback. The implications of our study provide a methodological framework for the analysis of reaching motions for selection and manipulation with novel 3D interaction metaphors and to successfully characterize visuo-haptic versus non-visuo-haptic physical reaches in virtual and real world situations.

Previous research has demonstrated that self-avatar representation of the user enhances the sense of presence \cite{LNW+03} and even a static notion of an avatar can improve distance estimation in far distances \cite{RIK+08,MCW+10}. In the present study, we will investigate the effect of visual fidelity of the self-avatar in enhancing the user's depth judgments. Participants will be randomly assigned to one of the three conditions: i) high-fidelity self-avatar (realistic limb matching the size and scale of the participant’s limbs), ii) medium-fidelity (abstract rendering of joint and limb location only), and iii) low-fidelity (abstract rendering of joint locations only). It is expected that participant depth judgments will improve regardless of the calibration condition. Further, it is expected that the high-fidelity self-avatar of the participant’s arm will significantly enhance depth perception in the IVE by providing a more realistic proprioceptive cue as compared to medium and low-fidelity avatar. 






